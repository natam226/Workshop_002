{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30cc4309",
   "metadata": {},
   "source": [
    "# **_004_API Data Extraction_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ce227a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import random\n",
    "import html\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5755b0bb",
   "metadata": {},
   "source": [
    "In this part we clean a list of artist names and retrieve additional data from Wikidata (like country, awards, gender, and number of albums)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b1d667",
   "metadata": {},
   "source": [
    "- Step 1: Clean the artist names\n",
    "\n",
    "    - Function limpiar_nombre(nombre):\n",
    "\n",
    "    - Removes unwanted characters like backslashes, quotes, commas, slashes, and ampersands.\n",
    "\n",
    "    - Trims spaces and returns a clean name or None if empty.\n",
    "\n",
    "- Step 2: Load and preprocess the data\n",
    "\n",
    "    - Read the CSV file artists.csv (without header).\n",
    "\n",
    "    - Apply cleaning function to each name.\n",
    "\n",
    "    - Remove duplicates and sort the list.\n",
    "\n",
    "- Step 3: Prepare SPARQL query\n",
    "\n",
    "    - Function construir_query_sparql(artistas):\n",
    "\n",
    "    - Builds a SPARQL query dynamically for a list of artists.\n",
    "\n",
    "    - Retrieves:\n",
    "\n",
    "        - Artist label\n",
    "\n",
    "        - Country\n",
    "\n",
    "        - Award\n",
    "\n",
    "        - Gender\n",
    "\n",
    "        - Album count\n",
    "\n",
    "    - Uses optional clauses to get data only if available.\n",
    "\n",
    "    - Groups the results to avoid duplicates.\n",
    "\n",
    "- Step 4: Query Wikidata API\n",
    "\n",
    "    - Function obtener_datos_wikidata(artistas_batch):\n",
    "\n",
    "    -  Sends the SPARQL query to Wikidataâ€™s endpoint.\n",
    "\n",
    "    - Handles HTTP errors gracefully.\n",
    "\n",
    "- Step 5: Process artists in batches\n",
    "\n",
    "    - To respect the query size limit:\n",
    "\n",
    "    - Process artists in batches of up to 80.\n",
    "\n",
    "    - Adjusts the batch size if the query exceeds the byte limit (60,000 bytes).\n",
    "\n",
    "    - Uses tqdm for progress visualization.\n",
    "\n",
    "- Step 6: Store the results\n",
    "\n",
    "    - Collects the retrieved data into a list of dictionaries (results).\n",
    "\n",
    "    - For each row returned, stores:\n",
    "\n",
    "        - Artist name\n",
    "\n",
    "        - Country\n",
    "\n",
    "        - Award\n",
    "\n",
    "        - Gender\n",
    "\n",
    "        - Album count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf2ca21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Total artistas Ãºnicos: 31437\n",
      "ðŸš€ Consultando Wikidata...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ”Ž Batches SPARQL: 31440it [10:59, 47.69it/s]                           \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "def limpiar_nombre(nombre):\n",
    "    if pd.isna(nombre) or not nombre.strip():\n",
    "        return None\n",
    "    nombre = nombre.replace(\"\\\\\", \"\").replace('\"', '').replace(\"'\", \"\")\n",
    "    nombre = nombre.replace(\",\", \"\").replace(\"/\", \" \").replace(\"&\", \"and\")\n",
    "    return nombre.strip()\n",
    "\n",
    "df = pd.read_csv(\"../data/artists.csv\", header=None, names=[\"raw\"])\n",
    "nombres_limpios = [limpiar_nombre(nombre) for nombre in df[\"raw\"]]\n",
    "artistas_unicos = sorted(set([nombre for nombre in nombres_limpios if nombre]))\n",
    "\n",
    "print(f\"Total artistas Ãºnicos: {len(artistas_unicos)}\")\n",
    "\n",
    "WIKIDATA_ENDPOINT = \"https://query.wikidata.org/sparql\"\n",
    "HEADERS = {\n",
    "    \"Accept\": \"application/sparql-results+json\",\n",
    "    \"User-Agent\": \"Workshop/1.0 (tucorreo@example.com)\"\n",
    "}\n",
    "\n",
    "def construir_query_sparql(artistas):\n",
    "    values = \"\\n\".join([f'\"{nombre}\"@en' for nombre in artistas])\n",
    "    query = f\"\"\"\n",
    "    SELECT ?artistLabel ?countryLabel ?awardLabel ?genderLabel (COUNT(?album) AS ?albumCount) WHERE {{\n",
    "      VALUES ?name {{ {values} }}\n",
    "      ?artist rdfs:label ?name.\n",
    "      OPTIONAL {{ ?artist wdt:P166 ?award. }}\n",
    "      OPTIONAL {{ ?artist wdt:P27 ?country. }}\n",
    "      OPTIONAL {{ ?artist wdt:P21 ?gender. }}\n",
    "\n",
    "      OPTIONAL {{\n",
    "        ?album wdt:P31 wd:Q482994.\n",
    "        ?album wdt:P175 ?artist.\n",
    "      }}\n",
    "\n",
    "      SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }}\n",
    "    }}\n",
    "    GROUP BY ?artistLabel ?countryLabel ?awardLabel ?genderLabel\n",
    "    \"\"\"\n",
    "    return query\n",
    "\n",
    "def obtener_datos_wikidata(artistas_batch):\n",
    "    query = construir_query_sparql(artistas_batch)\n",
    "    try:\n",
    "        response = requests.post(WIKIDATA_ENDPOINT, data={\"query\": query}, headers=HEADERS)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"Error en SPARQL:\", e)\n",
    "        return None\n",
    "\n",
    "print(\"Consultando Wikidata...\")\n",
    "\n",
    "MAX_QUERY_SIZE = 60000\n",
    "results = []\n",
    "i = 0\n",
    "\n",
    "with tqdm(total=len(artistas_unicos), desc=\"ðŸ”Ž Batches SPARQL\") as pbar:\n",
    "    while i < len(artistas_unicos):\n",
    "        batch_size = 80\n",
    "        batch_success = False\n",
    "\n",
    "        while batch_size > 0 and not batch_success:\n",
    "            batch = artistas_unicos[i:i+batch_size]\n",
    "            query = construir_query_sparql(batch)\n",
    "            if len(query.encode(\"utf-8\")) > MAX_QUERY_SIZE:\n",
    "                batch_size -= 5\n",
    "                continue\n",
    "\n",
    "            data = obtener_datos_wikidata(batch)\n",
    "            if data:\n",
    "                for row in data[\"results\"][\"bindings\"]:\n",
    "                    results.append({\n",
    "                        \"artist\": row.get(\"artistLabel\", {}).get(\"value\", \"\"),\n",
    "                        \"country\": row.get(\"countryLabel\", {}).get(\"value\", \"\"),\n",
    "                        \"award\": row.get(\"awardLabel\", {}).get(\"value\", \"No awards\"),\n",
    "                        \"gender\": row.get(\"genderLabel\", {}).get(\"value\", \"Unknown\"),\n",
    "                        \"album_count\": row.get(\"albumCount\", {}).get(\"value\", \"0\")\n",
    "                    })\n",
    "                batch_success = True\n",
    "                i += batch_size\n",
    "                pbar.update(batch_size)\n",
    "                time.sleep(0.8)\n",
    "            else:\n",
    "                batch_size = batch_size // 2\n",
    "\n",
    "        if not batch_success:\n",
    "            print(f\"Saltando artista en Ã­ndice {i}: {artistas_unicos[i]}\")\n",
    "            i += 1\n",
    "            pbar.update(1)\n",
    "\n",
    "columnas_ordenadas = [\"artist\", \"country\", \"award\", \"gender\", \"album_count\"]\n",
    "df_result = pd.DataFrame(results, columns=columnas_ordenadas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61f0511",
   "metadata": {},
   "source": [
    "Finally, the data is split in two and then the extracted data is saved in two different files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36eacac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Archivo 'api_data_part1.csv' creado correctamente.\n",
      "âœ… Archivo 'api_data_part2.csv' creado correctamente.\n"
     ]
    }
   ],
   "source": [
    "halfway = len(df_result) // 2\n",
    "\n",
    "df_result.iloc[:halfway].to_csv(\"../data/api_data_part1.csv\", index=False)\n",
    "print(\"Archivo 'api_data_part1.csv' creado correctamente.\")\n",
    "\n",
    "df_result.iloc[halfway:].to_csv(\"../data/api_data_part2.csv\", index=False)\n",
    "print(\"Archivo 'api_data_part2.csv' creado correctamente.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
